{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import re\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text pre-processing functions\n",
    "def clean_text(text):\n",
    "    text = text.replace('\\nBULLET::::', ' ')\n",
    "    text = text.replace('BULLET::::-', ' ')\n",
    "    text = text.replace('BULLET::::', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.replace('\\n\\n', ' ')\n",
    "    text = text.replace(r',', '')\n",
    "    text = text.replace('.', '')\n",
    "    text = text.replace(' - ', '')\n",
    "    text = text.replace('-', '')\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "    text = text.replace('Page', ' ')\n",
    "    text = text.replace(':', ' ')\n",
    "    text = text.replace(';', ' ')\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    text = text.replace('(', '')\n",
    "    text = text.replace('[', '')\n",
    "    text = text.replace(']', '')\n",
    "    text = text.replace(')', '')\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\d+.', ' ', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def get_tamil_stop_words():\n",
    "    swdf1 = pd.read_csv(\"data/TamilNLP_TamilStopWords.txt\",  header=None) \n",
    "    swdf2 = pd.read_csv(\"data/custom_tamil_stop_words.txt\",  header=None) \n",
    "    sw1 = swdf1[0].tolist()\n",
    "    sw2 = swdf2[0].tolist()\n",
    "    tamil_stop_words = list(set(sw1 + sw2))\n",
    "    return tamil_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16250, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Tamil wiki lablled data\n",
    "df = pd.read_csv(\"data/cleaned_tamil_wiki_text_data.csv\")\n",
    "df = df.sample(frac=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2157, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many articles are more than 300 words, \n",
    "df['word_count'] = df.text.apply(lambda x: len(str(x).split()))\n",
    "more_than_300 =  df['word_count'] > 300\n",
    "df3 = df[more_than_300]\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "train_set,test_set = train_test_split(df,test_size=0.15,random_state=50)\n",
    "X_train = train_set.text\n",
    "X_test = test_set.text\n",
    "y_train = train_set.parent_category\n",
    "y_test = test_set.parent_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403044\n"
     ]
    }
   ],
   "source": [
    "# convert text into embedded vectors\n",
    "maxlen = 325\n",
    "tokenizer = Tokenizer(num_words=10000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the text to a constant length\n",
    "X_train = pad_sequences(X_train_tokens, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test_tokens, padding='post', maxlen=maxlen)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['சமூகம்', 'மானிடவியல்', 'சமயம்', 'கணினியியல்', 'உயிரியல்', 'மொழி',\n",
       "       'ஊடகவியல்', 'கணிதம்', 'இயற்பியல்', 'வேதியியல்', 'திரைப்படம்', 'சட்டம்',\n",
       "       'கல்வி', 'பண்பாடு', 'வரலாறு', 'கட்டிடக்கலை', 'அரசியல்', 'புவியியல்',\n",
       "       'வணிகவியல்', 'உளவியல்', 'இசை', 'வானியல்', 'நலம்', 'இலக்கியம்',\n",
       "       'தொழினுட்பம்'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the labels, note the labels_index so we can recover the labels when we predict\n",
    "category_encoding_test = pd.factorize(y_test)\n",
    "category_encoding = pd.factorize(y_train)\n",
    "labels, labels_index = category_encoding\n",
    "labels_test, labels_index_test = category_encoding_test\n",
    "y_train_encoded = keras.utils.to_categorical(labels)\n",
    "y_test_encoded = keras.utils.to_categorical(labels_test)\n",
    "labels_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build embeddings\n",
    "embeddings_dictionary = dict()\n",
    "## Note part of github repo, pls download it from \n",
    "## https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md\n",
    "glove_file = open('data/cc.ta.300.vec', encoding=\"utf8\")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n",
    "\n",
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 325, 300)          120913200 \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 321, 100)          150100    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 125)               12625     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 25)                3150      \n",
      "=================================================================\n",
      "Total params: 121,079,075\n",
      "Trainable params: 121,079,075\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, SpatialDropout1D, MaxPooling1D, Dropout, Activation, GlobalMaxPooling1D\n",
    "model_conv = Sequential()\n",
    "model_conv.add(Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxlen))\n",
    "model_conv.add(Conv1D(100, 5))\n",
    "#model_conv.add(Flatten())\n",
    "model_conv.add(GlobalMaxPooling1D())\n",
    "model_conv.add(Dense(125, activation='relu'))\n",
    "model_conv.add(Dense(25, activation='softmax'))\n",
    "model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Applications/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 120913200 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11049 samples, validate on 2763 samples\n",
      "Epoch 1/12\n",
      "11049/11049 [==============================] - 64s 6ms/step - loss: 3.1073 - acc: 0.1254 - val_loss: 2.9745 - val_acc: 0.1737\n",
      "Epoch 2/12\n",
      "11049/11049 [==============================] - 62s 6ms/step - loss: 2.7789 - acc: 0.3167 - val_loss: 2.6716 - val_acc: 0.3297\n",
      "Epoch 3/12\n",
      "11049/11049 [==============================] - 56s 5ms/step - loss: 2.3568 - acc: 0.4602 - val_loss: 2.2905 - val_acc: 0.4191\n",
      "Epoch 4/12\n",
      "11049/11049 [==============================] - 63s 6ms/step - loss: 1.8604 - acc: 0.5634 - val_loss: 1.9337 - val_acc: 0.4716\n",
      "Epoch 5/12\n",
      "11049/11049 [==============================] - 62s 6ms/step - loss: 1.4005 - acc: 0.6545 - val_loss: 1.7006 - val_acc: 0.5270\n",
      "Epoch 6/12\n",
      "11049/11049 [==============================] - 61s 6ms/step - loss: 1.0467 - acc: 0.7345 - val_loss: 1.5635 - val_acc: 0.5552\n",
      "Epoch 7/12\n",
      "11049/11049 [==============================] - 66s 6ms/step - loss: 0.7729 - acc: 0.8127 - val_loss: 1.5002 - val_acc: 0.5657\n",
      "Epoch 8/12\n",
      "11049/11049 [==============================] - 60s 5ms/step - loss: 0.5565 - acc: 0.8745 - val_loss: 1.4710 - val_acc: 0.5841\n",
      "Epoch 9/12\n",
      "11049/11049 [==============================] - 62s 6ms/step - loss: 0.4014 - acc: 0.9156 - val_loss: 1.4759 - val_acc: 0.5867\n",
      "Epoch 10/12\n",
      "11049/11049 [==============================] - 63s 6ms/step - loss: 0.2938 - acc: 0.9367 - val_loss: 1.4841 - val_acc: 0.5885\n",
      "Epoch 11/12\n",
      "11049/11049 [==============================] - 59s 5ms/step - loss: 0.2289 - acc: 0.9443 - val_loss: 1.5089 - val_acc: 0.5925\n",
      "Epoch 12/12\n",
      "11049/11049 [==============================] - 59s 5ms/step - loss: 0.1886 - acc: 0.9513 - val_loss: 1.5334 - val_acc: 0.5907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7c2d1e5390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "epochs = 12\n",
    "batch_size = 650\n",
    "model_conv.fit(X_train, y_train_encoded, validation_split=0.2, epochs = epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.230517\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_conv.evaluate(X_test, y_test_encoded, verbose=2)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tamil_stop_words = get_tamil_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>parent_category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>சுமத்ரான் காண்டாமிருக இனம் முற்றிலும் அழிந்துவ...</td>\n",
       "      <td>https://www.bbc.com/tamil/science-50534560</td>\n",
       "      <td>உயிரியல்</td>\n",
       "      <td>மனிதர்களின் வேட்டைகளினாலும் வாழிட பரப்பு அழிப்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>பிளாஸ்டிக்கில் இருந்து எண்ணெய் எடுக்கும் முயற்...</td>\n",
       "      <td>https://www.bbc.com/tamil/global-50199538</td>\n",
       "      <td>வேதியியல்</td>\n",
       "      <td>ஆய்வகத்தில் உலைக்கலனை கென்னத் போயிப்பெல்மெயர் ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0         NaN  சுமத்ரான் காண்டாமிருக இனம் முற்றிலும் அழிந்துவ...   \n",
       "1         NaN  பிளாஸ்டிக்கில் இருந்து எண்ணெய் எடுக்கும் முயற்...   \n",
       "\n",
       "                                       source parent_category  \\\n",
       "0  https://www.bbc.com/tamil/science-50534560        உயிரியல்   \n",
       "1   https://www.bbc.com/tamil/global-50199538       வேதியியல்   \n",
       "\n",
       "                                                text  \n",
       "0  மனிதர்களின் வேட்டைகளினாலும் வாழிட பரப்பு அழிப்...  \n",
       "1  ஆய்வகத்தில் உலைக்கலனை கென்னத் போயிப்பெல்மெயர் ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load external validation dataset\n",
    "validation_df = pd.read_csv(\"data/validation.csv\")\n",
    "validation_df['text'] = validation_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (tamil_stop_words)]))\n",
    "validation_df['text'] = validation_df.apply(lambda row: clean_text(row['text']),axis=1)\n",
    "validation_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 18,  8, 21, 22,  0,  0,  4,  9,  8, 13, 24,  1, 23])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict on external data validation set\n",
    "V_train = tokenizer.texts_to_sequences(validation_df.text)\n",
    "V_train = pad_sequences(V_train, padding='post', maxlen=maxlen)\n",
    "validation_predictions = model_conv.predict_classes(V_train)\n",
    "validation_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.35714285714285715\n"
     ]
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "j = 0\n",
    "correct = 0\n",
    "for y_1 in validation_predictions:\n",
    "    actual = validation_df.iloc[j].parent_category\n",
    "    predicted = labels_index[y_1]\n",
    "    #print (\"actual: \" + actual + \" \" + \"predicted: \" + predicted)\n",
    "    if actual == predicted:\n",
    "        correct = correct + 1\n",
    "    j = j + 1\n",
    "print(\"validation accuracy: \" + str(correct/j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parent_category                                                                 நலம்\n",
       "Unnamed: 1                                                                     12536\n",
       "Unnamed: 0                                                                    107760\n",
       "id                                                                            141616\n",
       "url                                       https://ta.wikipedia.org/wiki?curid=141616\n",
       "title                                                                 கருணைக்கிழங்கு\n",
       "text                               கருணைக்கிழங்கு கருனை அல்லது பூமி சல்லரைக்கிழங்...\n",
       "categories                                              ['காய்கறிகள்', 'கிழங்குகள்']\n",
       "parent_category.1                                                               நலம்\n",
       "parent_category_recursion_depth                                                    3\n",
       "is_cricketer                                                                   False\n",
       "is_temple                                                                      False\n",
       "word_count                                                                       534\n",
       "Name: 10740, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict one case as a sanity check\n",
    "test_id = 4565\n",
    "df.iloc[test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'நலம்'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict one case as a sanity check\n",
    "input_text = [df.iloc[test_id].text]\n",
    "seq = tokenizer.texts_to_sequences(input_text)\n",
    "padded = pad_sequences(seq, maxlen=maxlen)\n",
    "ynew = model_conv.predict(padded)\n",
    "y_classes = ynew.argmax(axis=-1)\n",
    "labels_index[y_classes[0]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
